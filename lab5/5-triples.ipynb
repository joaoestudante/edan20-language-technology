{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5: Extraction of subject–verb–object triples\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will extract relations from a parsed sentence involving two words or entities. You will start with pairs of words, namely a subject and its verb, and then extend your programs to triples: subject, verb, and object. In the triples, the subject and the object are the entities, and the verb represents the relation. \n",
    "\n",
    "$$\n",
    "\\text{Subject} \\xrightarrow[\\text{}]{\\text{Verb}} \\text{Object}\n",
    "$$\n",
    "\n",
    "The overall work is inspired by the _Prismatic_ knowledge base used in the IBM Watson system, where the subject, verb, and object triples are a way to extract knowledge from text.  See <a href=\"http://www.aclweb.org/anthology/W/W10/W10-0915.pdf\">this paper</a> for details. \n",
    "\n",
    "You will apply the extraction to multilingual texts: \n",
    "1. First you will use a parsed corpus of Swedish; and then\n",
    "2. You will apply it to other languages.\n",
    "            \n",
    "The objectives of this assignment are to:\n",
    "* Extract the subject–verb pairs from a parsed corpus\n",
    "* Extend the extraction to subject–verb–object triples\n",
    "* Understand how dependency parsing can help create a knowledge base\n",
    "* Write a short report of 1 to 2 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As corpora, you will use the Universal Dependencies: https://universaldependencies.org/.\n",
    "1. In the first part of the assignment, you will focus on Swedish as it is easier to understand for most students, and then \n",
    "2. Move on to all the other languages. \n",
    "\n",
    "You will only consider the training sets of each corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a parsed corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the latest version of Universal dependencies (2.6) and uncompress them. You have a local version in the `/usr/local/cs/EDAN20/` folder on LTH's machines;\n",
    "2. Go to the Swedish _Talbanken_ corpus;\n",
    "3. Read the CoNLL-U annotation here: https://universaldependencies.org/format.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will carry out the following steps and describe them in your report:\n",
    "\n",
    "1. Draw graphical representations of the two first Swedish sentences of the training set. You will include these drawings in your report;\n",
    "2. Visualize these sentences with this tool: http://spyysalo.github.io/conllu.js/ and check that you have the same results;\n",
    "3. Apply the dependency parser for Swedish of the <a href=\"http://vilde.cs.lth.se:9000/\">Langforia pipelines</a> to these sentences (only the text of each sentence). You will have to select Swedish and activate both `Token` and `DependencyRelation`. Link to Lanforia pipelines: <a href=\"http://vilde.cs.lth.se:9000/\">http://vilde.cs.lth.se:9000/</a>. You will describe possible differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swedish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will extract all the subject–verb pairs and the subject–verb–object triples from the Swedish _Talbanken_ training corpus. To start the program, you can use the CoNLL-U reader available in the cells below.\n",
    "This program works for the other corpora. You can also program a reader yourself starting from the one you used to read the CoNLL 2000 format in the fourth lab or from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the corpus locations you will use. You may have to adjust `ud_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_path = 'ud-treebanks-v2.6/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sv = ud_path + 'UD_Swedish-Talbanken/sv_talbanken-ud-train.conllu'\n",
    "path_fr = ud_path + 'UD_French-GSD/fr_gsd-ud-train.conllu'\n",
    "path_ru = ud_path + 'UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu'\n",
    "path_en = ud_path + 'UD_English-EWT/en_ewt-ud-train.conllu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names of the CoNLL-U corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_u = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to read the CoNLL-U files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file).read().strip()\n",
    "    sentences = f.split('\\n\\n')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentence where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    root_values = ['0', 'ROOT', 'ROOT', 'ROOT', 'ROOT', 'ROOT', '0', 'ROOT', '0', 'ROOT']\n",
    "    start = [dict(zip(column_names, root_values))]\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split('\\t'))) for row in rows if row[0] != '#']\n",
    "        sentence = start + sentence\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the Swedish Talbanken corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_sentences(path_sv)\n",
    "formatted_corpus = split_rows(sentences, column_names_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4303"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(formatted_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsed sentence: _Genom skattereformen införs individuell beskattning (särbeskattning) av arbetsinkomster._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': '0',\n",
       "  'FORM': 'ROOT',\n",
       "  'LEMMA': 'ROOT',\n",
       "  'UPOS': 'ROOT',\n",
       "  'XPOS': 'ROOT',\n",
       "  'FEATS': 'ROOT',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'ROOT',\n",
       "  'DEPS': '0',\n",
       "  'MISC': 'ROOT'},\n",
       " {'ID': '1',\n",
       "  'FORM': 'Genom',\n",
       "  'LEMMA': 'genom',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '2',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '2:case',\n",
       "  'MISC': '_'},\n",
       " {'ID': '2',\n",
       "  'FORM': 'skattereformen',\n",
       "  'LEMMA': 'skattereform',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|DEF|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Def|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'obl',\n",
       "  'DEPS': '3:obl:genom',\n",
       "  'MISC': '_'},\n",
       " {'ID': '3',\n",
       "  'FORM': 'införs',\n",
       "  'LEMMA': 'införa',\n",
       "  'UPOS': 'VERB',\n",
       "  'XPOS': 'VB|PRS|SFO',\n",
       "  'FEATS': 'Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Pass',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'root',\n",
       "  'DEPS': '0:root',\n",
       "  'MISC': '_'},\n",
       " {'ID': '4',\n",
       "  'FORM': 'individuell',\n",
       "  'LEMMA': 'individuell',\n",
       "  'UPOS': 'ADJ',\n",
       "  'XPOS': 'JJ|POS|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Degree=Pos|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'amod',\n",
       "  'DEPS': '5:amod',\n",
       "  'MISC': '_'},\n",
       " {'ID': '5',\n",
       "  'FORM': 'beskattning',\n",
       "  'LEMMA': 'beskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'nsubj:pass',\n",
       "  'DEPS': '3:nsubj:pass',\n",
       "  'MISC': '_'},\n",
       " {'ID': '6',\n",
       "  'FORM': '(',\n",
       "  'LEMMA': '(',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " {'ID': '7',\n",
       "  'FORM': 'särbeskattning',\n",
       "  'LEMMA': 'särbeskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'appos',\n",
       "  'DEPS': '5:appos',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " {'ID': '8',\n",
       "  'FORM': ')',\n",
       "  'LEMMA': ')',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': '_'},\n",
       " {'ID': '9',\n",
       "  'FORM': 'av',\n",
       "  'LEMMA': 'av',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '10',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '10:case',\n",
       "  'MISC': '_'},\n",
       " {'ID': '10',\n",
       "  'FORM': 'arbetsinkomster',\n",
       "  'LEMMA': 'arbetsinkomst',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|PLU|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Plur',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'nmod',\n",
       "  'DEPS': '5:nmod:av',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " {'ID': '11',\n",
       "  'FORM': '.',\n",
       "  'LEMMA': '.',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'MAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '3:punct',\n",
       "  'MISC': '_'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the lists in dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease the processing of some corpora, you will use a dictionary represention of the sentences. The keys will be the `ID` values. We do this because `ID` is not necessarily a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(formatted_corpus):\n",
    "    \"\"\"\n",
    "    Converts each sentence from a list of words to a dictionary where the keys are id\n",
    "    :param formatted_corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    formatted_corpus_dict = []\n",
    "    for sentence in formatted_corpus:\n",
    "        sentence_dict = {}\n",
    "        for word in sentence:\n",
    "            sentence_dict[word['ID']] = word\n",
    "        formatted_corpus_dict.append(sentence_dict)\n",
    "    return formatted_corpus_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'ID': '0',\n",
       "  'FORM': 'ROOT',\n",
       "  'LEMMA': 'ROOT',\n",
       "  'UPOS': 'ROOT',\n",
       "  'XPOS': 'ROOT',\n",
       "  'FEATS': 'ROOT',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'ROOT',\n",
       "  'DEPS': '0',\n",
       "  'MISC': 'ROOT'},\n",
       " '1': {'ID': '1',\n",
       "  'FORM': 'Genom',\n",
       "  'LEMMA': 'genom',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '2',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '2:case',\n",
       "  'MISC': '_'},\n",
       " '2': {'ID': '2',\n",
       "  'FORM': 'skattereformen',\n",
       "  'LEMMA': 'skattereform',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|DEF|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Def|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'obl',\n",
       "  'DEPS': '3:obl:genom',\n",
       "  'MISC': '_'},\n",
       " '3': {'ID': '3',\n",
       "  'FORM': 'införs',\n",
       "  'LEMMA': 'införa',\n",
       "  'UPOS': 'VERB',\n",
       "  'XPOS': 'VB|PRS|SFO',\n",
       "  'FEATS': 'Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Pass',\n",
       "  'HEAD': '0',\n",
       "  'DEPREL': 'root',\n",
       "  'DEPS': '0:root',\n",
       "  'MISC': '_'},\n",
       " '4': {'ID': '4',\n",
       "  'FORM': 'individuell',\n",
       "  'LEMMA': 'individuell',\n",
       "  'UPOS': 'ADJ',\n",
       "  'XPOS': 'JJ|POS|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Degree=Pos|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'amod',\n",
       "  'DEPS': '5:amod',\n",
       "  'MISC': '_'},\n",
       " '5': {'ID': '5',\n",
       "  'FORM': 'beskattning',\n",
       "  'LEMMA': 'beskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'nsubj:pass',\n",
       "  'DEPS': '3:nsubj:pass',\n",
       "  'MISC': '_'},\n",
       " '6': {'ID': '6',\n",
       "  'FORM': '(',\n",
       "  'LEMMA': '(',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " '7': {'ID': '7',\n",
       "  'FORM': 'särbeskattning',\n",
       "  'LEMMA': 'särbeskattning',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|SIN|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Sing',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'appos',\n",
       "  'DEPS': '5:appos',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " '8': {'ID': '8',\n",
       "  'FORM': ')',\n",
       "  'LEMMA': ')',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'PAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '5:punct',\n",
       "  'MISC': '_'},\n",
       " '9': {'ID': '9',\n",
       "  'FORM': 'av',\n",
       "  'LEMMA': 'av',\n",
       "  'UPOS': 'ADP',\n",
       "  'XPOS': 'PP',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '10',\n",
       "  'DEPREL': 'case',\n",
       "  'DEPS': '10:case',\n",
       "  'MISC': '_'},\n",
       " '10': {'ID': '10',\n",
       "  'FORM': 'arbetsinkomster',\n",
       "  'LEMMA': 'arbetsinkomst',\n",
       "  'UPOS': 'NOUN',\n",
       "  'XPOS': 'NN|UTR|PLU|IND|NOM',\n",
       "  'FEATS': 'Case=Nom|Definite=Ind|Gender=Com|Number=Plur',\n",
       "  'HEAD': '5',\n",
       "  'DEPREL': 'nmod',\n",
       "  'DEPS': '5:nmod:av',\n",
       "  'MISC': 'SpaceAfter=No'},\n",
       " '11': {'ID': '11',\n",
       "  'FORM': '.',\n",
       "  'LEMMA': '.',\n",
       "  'UPOS': 'PUNCT',\n",
       "  'XPOS': 'MAD',\n",
       "  'FEATS': '_',\n",
       "  'HEAD': '3',\n",
       "  'DEPREL': 'punct',\n",
       "  'DEPS': '3:punct',\n",
       "  'MISC': '_'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_corpus_dict = convert_to_dict(formatted_corpus)\n",
    "formatted_corpus_dict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the subject-verb pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will extract the subject-verb pairs, where you will set the words in lowercase. In the second sentence of the corpus, this corresponds to `(beskattning, införs)`. You will call the function `extract_pairs(formatted_corpus_dict)` and and you will store the results in a `pairs_sv` variable. All the corpora in the universal dependencies format use the same function names: `nsubj` and `obj` for the subject and direct object.\n",
    "\n",
    "You can use the algorithm you want. However, here are some hints on the results:\n",
    "* You will extract all the subject-verb pairs in the corpus. In the extraction, just check the function between two words. Do not check if the part of speech is a verb or a noun in the pair. You will also ignore the possible function suffixes as in `nsubj:pass`, where `pass` means passive.\n",
    "* You will return the results as Python's dictionaries, where the key will be the pair and the value, the count, as for instance `{(beskattning, införs): 1}`. Be sure you understand the Python dictionaries and note that you can use tuples as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(corpus_dict):\n",
    "    result = Counter()\n",
    "    \n",
    "    for sentence in corpus_dict:\n",
    "        for _id in sentence:\n",
    "            word = sentence[_id]\n",
    "            head_id = word['HEAD']\n",
    "            deprel = word[\"DEPREL\"].split(\":\")\n",
    "            if deprel[0] == \"nsubj\":\n",
    "                result[(word['FORM'].lower(), sentence[head_id]['FORM'].lower())] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_sv = extract_pairs(formatted_corpus_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will compute the total number of subject-verb pairs. You should find 6,083 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6083"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([pairs_sv[pair] for pair in pairs_sv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the most frequent pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will sort your pairs by frequency and by lexical order of the pairs and store the five most frequent pairs in the `freq_pairs_sv` variable as in:\n",
    "```\n",
    "freq_pairs_sv = [(('som', 'har'), 45),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "Here are the frequencies you should find:\n",
    "```\n",
    "45\n",
    "19\n",
    "19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the experiments, we will keep the `nbest` most frequent. In the first experiments, we set `nbest` to 3 first. We will set it to 5 in the last experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pairs = sorted(pairs_sv, key=lambda x: (-pairs_sv[x], x))\n",
    "freq_pairs_sv = [(pair, pairs_sv[pair]) for pair in sorted_pairs][:nbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('som', 'har'), 45), (('du', 'får'), 19), (('vi', 'har'), 19)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pairs_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the subject-verb-object triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now extract all the subject–verb–object triples of the corpus. The object function uses the `obj` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triples(corpus_dict):\n",
    "    result = Counter()\n",
    "    \n",
    "    for sentence in corpus_dict:\n",
    "        subject_verb_object = []\n",
    "        for _id in sentence:\n",
    "            word = sentence[_id]\n",
    "            head_id = word['HEAD']\n",
    "            deprel = word[\"DEPREL\"].split(\":\")\n",
    "            \n",
    "            if deprel[0] == \"nsubj\":\n",
    "                updated = False\n",
    "                head_form = sentence[head_id]['FORM'].lower()\n",
    "                for svo in subject_verb_object:\n",
    "                    if svo[\"verb\"] == head_form and not svo[\"subject\"]:\n",
    "                        svo[\"subject\"] = word['FORM'].lower()\n",
    "                        updated = True\n",
    "                        break\n",
    "                if not updated:\n",
    "                    subject_verb_object.append({\"subject\":word[\"FORM\"].lower(), \"verb\":head_form, \"object\":\"\"})\n",
    "                    \n",
    "            elif deprel[0] == \"obj\":\n",
    "                updated = False\n",
    "                head_form = sentence[head_id]['FORM'].lower()\n",
    "                for svo in subject_verb_object:\n",
    "                    if svo[\"verb\"] == head_form and not svo[\"object\"]:\n",
    "                        svo[\"object\"] = word['FORM'].lower()\n",
    "                        updated = True\n",
    "                        break\n",
    "                if not updated:\n",
    "                    subject_verb_object.append({\"subject\":\"\", \"verb\":head_form, \"object\":word[\"FORM\"].lower()})\n",
    "\n",
    "        for el in subject_verb_object:\n",
    "            if el[\"subject\"] and el[\"verb\"] and el[\"object\"]:\n",
    "                result[(el[\"subject\"], el[\"verb\"], el[\"object\"])] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_sv = extract_triples(formatted_corpus_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the total number of triples. You should find 2054 triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2054"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([triples_sv[triple] for triple in triples_sv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the most frequent triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will sort your triples by frequency and by lexical order of the pairs and store the three most frequent triples in the `freq_triples_sv` variable as in:\n",
    "```\n",
    "freq_triples_sv = [(('man', 'vänder', 'sig'), 14),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "Here are the frequencies you should find:\n",
    "```\n",
    "14\n",
    "5\n",
    "3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_triples = sorted(triples_sv, key=lambda x: (-triples_sv[x], x))\n",
    "freq_triples_sv = [(triple, triples_sv[triple]) for triple in sorted_triples][:nbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('man', 'vänder', 'sig'), 14),\n",
       " (('det', 'rör', 'sig'), 5),\n",
       " (('man', 'söker', 'arbete'), 3)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_triples_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your program is working on Swedish, you will apply it to all the other languages in universal dependencies. The code below returns all the files from a folder with a suffix. Here we consider the training files only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    Recursive version\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        path = dir + '/' + file\n",
    "        if os.path.isdir(path):\n",
    "            files += get_files(path, suffix)\n",
    "        elif os.path.isfile(path) and file.endswith(suffix):\n",
    "            files.append(path)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ud-treebanks-v2.6//UD_Scottish_Gaelic-ARCOSG/gd_arcosg-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Welsh-CCG/cy_ccg-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Greek-GDT/el_gdt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Naija-NSC/pcm_nsc-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Coptic-Scriptorium/cop_scriptorium-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Ukrainian-IU/uk_iu-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Vietnamese-VTB/vi_vtb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Russian-GSD/ru_gsd-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Swedish_Sign_Language-SSLC/swl_sslc-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Swedish-Talbanken/sv_talbanken-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Italian-VIT/it_vit-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_North_Sami-Giella/sme_giella-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Latin-Perseus/la_perseus-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Hindi-HDTB/hi_hdtb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Maltese-MUDT/mt_mudt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Catalan-AnCora/ca_ancora-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Czech-PDT/cs_pdt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Dutch-Alpino/nl_alpino-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_English-GUM/en_gum-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Irish-IDT/ga_idt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Romanian-RRT/ro_rrt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Livvi-KKPP/olo_kkpp-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Serbian-SET/sr_set-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Spanish-AnCora/es_ancora-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Upper_Sorbian-UFAL/hsb_ufal-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Polish-PDB/pl_pdb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Italian-PoSTWITA/it_postwita-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Telugu-MTG/te_mtg-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Finnish-TDT/fi_tdt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Chinese-GSD/zh_gsd-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Urdu-UDTB/ur_udtb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Japanese-GSD/ja_gsd-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_English-GUMReddit/en_gumreddit-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Dutch-LassySmall/nl_lassysmall-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Old_French-SRCMF/fro_srcmf-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Danish-DDT/da_ddt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Sanskrit-Vedic/sa_vedic-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Ancient_Greek-PROIEL/grc_proiel-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Romanian-Nonstandard/ro_nonstandard-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Czech-CLTT/cs_cltt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Croatian-SET/hr_set-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Portuguese-Bosque/pt_bosque-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Wolof-WTB/wo_wtb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Russian-Taiga/ru_taiga-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Hungarian-Szeged/hu_szeged-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Hindi_English-HIENCS/qhe_hiencs-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Old_Church_Slavonic-PROIEL/cu_proiel-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Buryat-BDT/bxr_bdt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Afrikaans-AfriBooms/af_afribooms-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Norwegian-Nynorsk/no_nynorsk-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Bulgarian-BTB/bg_btb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_French-ParTUT/fr_partut-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Norwegian-NynorskLIA/no_nynorsklia-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Arabic-PADT/ar_padt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Marathi-UFAL/mr_ufal-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Estonian-EWT/et_ewt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Armenian-ArmTDP/hy_armtdp-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Latin-LLCT/la_llct-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_French-FTB/fr_ftb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Spanish-GSD/es_gsd-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Norwegian-Bokmaal/no_bokmaal-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Slovenian-SST/sl_sst-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Latvian-LVTB/lv_lvtb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Lithuanian-HSE/lt_hse-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Persian-Seraji/fa_seraji-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_English-ESL/en_esl-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Latin-PROIEL/la_proiel-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Gothic-PROIEL/got_proiel-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Slovenian-SSJ/sl_ssj-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_English-ParTUT/en_partut-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Portuguese-GSD/pt_gsd-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Korean-GSD/ko_gsd-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Kazakh-KTB/kk_ktb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_German-HDT/de_hdt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Basque-BDT/eu_bdt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Italian-TWITTIRO/it_twittiro-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Galician-TreeGal/gl_treegal-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Lithuanian-ALKSNIS/lt_alksnis-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Kurmanji-MG/kmr_mg-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Slovak-SNK/sk_snk-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_French-Sequoia/fr_sequoia-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Uyghur-UDT/ug_udt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Korean-Kaist/ko_kaist-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Ancient_Greek-Perseus/grc_perseus-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Italian-ISDT/it_isdt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Estonian-EDT/et_edt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Chinese-GSDSimp/zh_gsdsimp-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Old_Russian-RNC/orv_rnc-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Classical_Chinese-Kyoto/lzh_kyoto-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_English-LinES/en_lines-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Old_Russian-TOROT/orv_torot-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Swedish-LinES/sv_lines-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Turkish-IMST/tr_imst-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_German-GSD/de_gsd-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Arabic-NYUAD/ar_nyuad-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Latin-ITTB/la_ittb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Hebrew-HTB/he_htb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_French-Spoken/fr_spoken-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Indonesian-GSD/id_gsd-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Belarusian-HSE/be_hse-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Finnish-FTB/fi_ftb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Czech-FicTree/cs_fictree-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Tamil-TTB/ta_ttb-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Czech-CAC/cs_cac-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Galician-CTG/gl_ctg-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_English-EWT/en_ewt-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_French-GSD/fr_gsd-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Italian-ParTUT/it_partut-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Japanese-BCCWJ/ja_bccwj-ud-train.conllu',\n",
       " 'ud-treebanks-v2.6//UD_Polish-LFG/pl_lfg-ud-train.conllu']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = get_files(ud_path, 'train.conllu')\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some corpora expand some tokens into multiwords. This is the case in French, Spanish, and German.\n",
    "        The table below shows examples of such expansions.\n",
    "        <table style=\"width:100%\">\n",
    "            <tr>\n",
    "                <th>French</th>\n",
    "                <th>Spanish</th>\n",
    "                <th>German</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><i>du</i>: de le\n",
    "                </td>\n",
    "                <td><i>del</i>: de el\n",
    "                </td>\n",
    "                <td><i>zur</i>: zu der\n",
    "                </td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td><i>des</i>: de les\n",
    "                </td>\n",
    "                <td><i>vámonos</i>: vamos nos\n",
    "                </td>\n",
    "                <td><i>im</i>: in dem\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        In the corpora, you have the original tokens as well as the multiwords as with <i>vámonos al mar</i>.\n",
    "        <pre>\n",
    "1-2 vámonos _\n",
    "1 vamos ir\n",
    "2 nos nosotros\n",
    "3-4 al _\n",
    "3 a a\n",
    "4 el el\n",
    "5 mar mar\n",
    "</pre>Read the format description for the details: [<a\n",
    "                href=\"http://universaldependencies.org/format.html\">CoNLL-U format</a>]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you represent the sentences as lists, the item indices are not reliable: In the format description,\n",
    "        the token at position 1 is <i>vamos</i> and not <i>vámonos</i>.\n",
    "        You have two ways to cope with this:\n",
    "1. Either remove all the lines that include a range in the `ID` field, or\n",
    "2. Encode the sentences as dictionaries (I felt this was preferable), where the keys are the `ID` numbers. This is what `convert_to_dict()` does. Here are the results for a sentence from the French CoNLL-U corpus:\n",
    "_Les iris du mâles sont jaunes toute l'année._ Note the `3-4` index and it expansion in `3`and `4`:\n",
    "```\n",
    "{'0': {'ID': '0',  'FORM': 'ROOT',  'LEMMA': 'ROOT',  'UPOS': 'ROOT',  'XPOS': 'ROOT',  'FEATS': 'ROOT',  'HEAD': '0',  'DEPREL': 'ROOT',  'DEPS': '0',  'MISC': 'ROOT'}, \n",
    "'1': {'ID': '1',  'FORM': 'Les',  'LEMMA': 'le',  'UPOS': 'DET',  'XPOS': '_',  'FEATS': 'Definite=Def|Gender=Masc|Number=Plur|PronType=Art',  'HEAD': '2',  'DEPREL': 'det',  'DEPS': '_',  'MISC': 'wordform=les'}, \n",
    "'2': {'ID': '2',  'FORM': 'iris',  'LEMMA': 'iris',  'UPOS': 'NOUN',  'XPOS': '_',  'FEATS': 'Gender=Masc|Number=Plur',  'HEAD': '7',  'DEPREL': 'nsubj',  'DEPS': '_',  'MISC': '_'}, \n",
    "'3-4': {'ID': '3-4',  'FORM': 'du',  'LEMMA': '_',  'UPOS': '_',  'XPOS': '_',  'FEATS': '_',  'HEAD': '_',  'DEPREL': '_',  'DEPS': '_',  'MISC': '_'}, \n",
    "'3': {'ID': '3',  'FORM': 'de',  'LEMMA': 'de',  'UPOS': 'ADP',  'XPOS': '_',  'FEATS': '_',  'HEAD': '5',  'DEPREL': 'case',  'DEPS': '_',  'MISC': '_'}, \n",
    "'4': {'ID': '4',  'FORM': 'le',  'LEMMA': 'le',  'UPOS': 'DET',  'XPOS': '_',  'FEATS': 'Definite=Def|Gender=Masc|Number=Sing|PronType=Art',  'HEAD': '5',  'DEPREL': 'det',  'DEPS': '_',  'MISC': '_'}, \n",
    "'5': {'ID': '5',  'FORM': 'mâles',  'LEMMA': 'mâle',  'UPOS': 'NOUN',  'XPOS': '_',  'FEATS': 'Gender=Masc|Number=Plur',  'HEAD': '2',  'DEPREL': 'nmod',  'DEPS': '_',  'MISC': '_'}, \n",
    "'6': {'ID': '6',  'FORM': 'sont',  'LEMMA': 'être',  'UPOS': 'AUX',  'XPOS': '_',  'FEATS': 'Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin',  'HEAD': '7',  'DEPREL': 'cop',  'DEPS': '_',  'MISC': '_'}, \n",
    "'7': {'ID': '7',  'FORM': 'jaunes',  'LEMMA': 'jaune',  'UPOS': 'ADJ',  'XPOS': '_',  'FEATS': 'Gender=Masc|Number=Plur',  'HEAD': '0',  'DEPREL': 'root',  'DEPS': '_',  'MISC': '_'}, \n",
    "'8': {'ID': '8',  'FORM': 'toute',  'LEMMA': 'tout',  'UPOS': 'ADJ',  'XPOS': '_',  'FEATS': 'Gender=Fem|Number=Sing',  'HEAD': '10',  'DEPREL': 'amod',  'DEPS': '_',  'MISC': '_'}, \n",
    "'9': {'ID': '9',  'FORM': \"l'\",  'LEMMA': 'le',  'UPOS': 'DET',  'XPOS': '_',  'FEATS': 'Definite=Def|Gender=Fem|Number=Sing|PronType=Art',  'HEAD': '10',  'DEPREL': 'det',  'DEPS': '_',  'MISC': 'SpaceAfter=No'}, \n",
    "'10': {'ID': '10',  'FORM': 'année',  'LEMMA': 'année',  'UPOS': 'NOUN',  'XPOS': '_',  'FEATS': 'Gender=Fem|Number=Sing',  'HEAD': '7',  'DEPREL': 'obl',  'DEPS': '_',  'MISC': 'SpaceAfter=No'}, \n",
    "'11': {'ID': '11',  'FORM': '.',  'LEMMA': '.',  'UPOS': 'PUNCT',  'XPOS': '_',  'FEATS': '_',  'HEAD': '7',  'DEPREL': 'punct',  'DEPS': '_',  'MISC': '_'}}\n",
    "```\n",
    "3. Some corpora have sentence numbers. You solve it by discarding lines starting with a `#`. This is already done in the CoNLL reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the pairs and triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `extract_pairs_and_triples(formatted_corpus_dict, nbest)` that extracts the `nbest` most frequent pairs and triples of a given corpus and returns two sorted lists of tuples: `frequent_pairs` and `frequent_triples`. You will sort them by frequency and then by alphabetical order of the pair or triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def extract_pairs_and_triples(formatted_corpus_dict, nbest):\n",
    "    corpus_pairs = extract_pairs(formatted_corpus_dict)\n",
    "    corpus_triples = extract_triples(formatted_corpus_dict)\n",
    "    \n",
    "    sorted_pairs = sorted(corpus_pairs, key=lambda x: (-corpus_pairs[x], x))\n",
    "    freq_pairs = [(pair, corpus_pairs[pair]) for pair in sorted_pairs][:nbest]\n",
    "    \n",
    "    sorted_triples = sorted(corpus_triples, key=lambda x: (-corpus_triples[x], x))\n",
    "    freq_triples = [(triple, corpus_triples[triple]) for triple in sorted_triples][:nbest]\n",
    "    \n",
    "    return freq_pairs, freq_triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your extractor on all the corpora. Note that some corpora have replaced the words by underscores as for one corpus n French. You need then to contact the provider to obtain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('e', 'bha'), 42), (('e', 'tha'), 40), (('e', 'robh'), 26)]\n",
      "[(('ni', 'ydyn'), 8), (('nhw', 'maen'), 7), (('chi', 'ydych'), 6)]\n",
      "[(('που', 'βρίσκονται'), 6), (('που', 'αφορά'), 5), (('πρόεδρος', 'δήλωσε'), 5)]\n",
      "[(('you', 'know'), 136), (('i', 'go'), 127), (('i', 'do'), 111)]\n",
      "[(('ϥ', 'ⲡⲉϫⲁ'), 51), (('ϥ', 'ϫⲱ'), 35), (('ϥ', 'ϫⲟⲟ'), 26)]\n",
      "[(('ви', 'знаєте'), 9), (('він', 'каже'), 9), (('він', 'міг'), 9)]\n",
      "[(('nhà', 'có'), 5), (('tôi', 'thấy'), 5), (('em', 'ở'), 4)]\n",
      "[(('население', 'составляет'), 15), (('население', 'составляло'), 13), (('он', 'получил'), 9)]\n",
      "[(('pro1', 'teckna.flyt'), 3), (('hund', 'varelse(vvb)+förflytta@p'), 2), (('hund', 'varelse(vvb)+rörelse@p'), 2)]\n",
      "[(('som', 'har'), 45), (('du', 'får'), 19), (('vi', 'har'), 19)]\n",
      "[(('che', 'ha'), 20), (('francesco', 'vinto'), 15), (('che', 'hanno'), 14)]\n",
      "[(('mun', 'oidnen'), 10), (('mun', 'ožžon'), 8), (('mun', 'oaidnán'), 6)]\n",
      "[(('qui', 'habet'), 5), (('spiritus', 'dicat'), 5), (('ego', 'vereor'), 3)]\n",
      "[(('उन्होंने', 'कहा'), 714), (('कहना', 'है'), 219), (('उन्होंने', 'बताया'), 101)]\n",
      "[(('bżonn', 'hemm'), 6), (('farrugia', 'semma'), 6), (('min', 'hawn'), 5)]\n",
      "[(('que', 'té'), 56), (('que', 've'), 55), (('objectiu', 'és'), 39)]\n",
      "[(('to', 'znamená'), 104), (('to', 'je'), 100), (('který', 'má'), 77)]\n",
      "[(('men', 'noemt'), 112), (('hij', 'komt'), 70), (('ik', 'wil'), 69)]\n",
      "[(('i', 'think'), 37), (('you', 'have'), 33), (('you', 'want'), 31)]\n",
      "[(('a', 'bhí'), 74), (('sé', 'tá'), 41), (('sé', 'bhí'), 39)]\n",
      "[(('care', 'pot'), 22), (('statele', 'pot'), 21), (('care', 'prezintă'), 16)]\n",
      "[(('häi', 'juoksi'), 1), (('häi', 'oppi'), 1), (('häi', 'osti'), 1)]\n",
      "[(('on', 'rekao'), 37), (('on', 'dodao'), 23), (('ministar', 'izjavio'), 13)]\n",
      "[(('que', 'tiene'), 59), (('que', 'supone'), 27), (('que', 'da'), 24)]\n",
      "[(('dokłady', 'njejsu'), 1), (('dopokazy', 'spisane'), 1), (('gero', 'da'), 1)]\n",
      "[(('mężczyzna', 'stoi'), 58), (('mężczyzna', 'siedzi'), 35), (('kobieta', 'stoi'), 31)]\n",
      "[(('#grillo', 'fa'), 20), (('governo', 'piace'), 17), (('che', 'fa'), 16)]\n",
      "[(('రాము', 'వెళ్ళేడు'), 9), (('కమల', 'వెళ్ళింది'), 6), (('నాకు', 'తెలుసు'), 5)]\n",
      "[(('joka', 'ottaa'), 19), (('joka', 'koskee'), 13), (('se', 'muutettuna'), 10)]\n",
      "[(('面積', '平方公里'), 52), (('人口', '人'), 41), (('高度', '米'), 14)]\n",
      "[(('انہوں', 'کہا'), 169), (('وہ', 'کر'), 43), (('انہوں', 'کیا'), 41)]\n",
      "[(('こと', '多い'), 14), (('こと', '可能'), 11), (('性', 'ある'), 9)]\n",
      "[(('_', '_'), 1058)]\n",
      "[(('ze', 'wint'), 19), (('ze', 'verslaat'), 11), (('hij', 'werd'), 8)]\n",
      "[(('il', 'fet'), 93), (('il', 'fait'), 41), (('il', 'vint'), 41)]\n",
      "[(('det', 'er'), 40), (('vi', 'har'), 31), (('han', 'siger'), 21)]\n",
      "[(('yaḥ', 'veda'), 76), (('sa', 'anuvyacalat'), 21), (('sa', 'bhavati'), 18)]\n",
      "[(('ἰησοῦς', 'εἶπεν'), 102), (('ὁ', 'εἶπεν'), 75), (('ἰησοῦς', 'λέγει'), 59)]\n",
      "[(('iisus', 'zise'), 114), (('el', 'zise'), 79), (('ce', 'zice'), 44)]\n",
      "[(('položka', 'obsahuje'), 96), (('jednotka', 'uvede'), 33), (('jednotky', 'povinny'), 31)]\n",
      "[(('ministar', 'izjavio'), 23), (('premijer', 'izjavio'), 15), (('koji', 'ima'), 11)]\n",
      "[(('que', 'tem'), 17), (('que', 'pode'), 12), (('que', 'está'), 11)]\n",
      "[(('nga', 'xam'), 32), (('ku', 'ne'), 28), (('mu', 'di'), 25)]\n",
      "[(('я', 'знаю'), 7), (('он', 'стал'), 6), (('я', 'вижу'), 6)]\n",
      "[(('akarata', 'erősebb'), 2), (('aki', 'ül'), 2), (('amely', 'teszi'), 2)]\n",
      "[(('_', '_'), 1792)]\n",
      "[(('онъ', 'рече'), 57), (('и҃съ', 'рече'), 39), (('и҃с', 'рече'), 29)]\n",
      "[(('баабай', 'бэрхэ'), 1), (('басаган', 'болоо'), 1), (('бороо', 'орожо'), 1)]\n",
      "[(('речь', 'идет'), 102), (('дело', 'том'), 86), (('мы', 'можем'), 70)]\n",
      "[(('ons', 'voortgaan'), 19), (('jy', 'doen'), 16), (('wat', 'het'), 15)]\n",
      "[(('vi', 'har'), 101), (('som', 'har'), 94), (('eg', 'trur'), 61)]\n",
      "[(('той', 'каза'), 12), (('това', 'стана'), 11), (('мвр', 'съобщи'), 8)]\n",
      "[(('personne', 'a'), 17), (('qui', 'concerne'), 11), ((\"c'\", 'est'), 7)]\n",
      "[(('eg', 'trur'), 52), (('du', 'ser'), 40), (('du', 'veit'), 40)]\n",
      "[(('هو', 'ما'), 28), (('التي', 'تم'), 26), (('الاتفاق', 'تم'), 23)]\n",
      "[(('तो', 'म्हणाला'), 9), (('ती', 'म्हणाली'), 6), (('राजा', 'म्हणाला'), 5)]\n",
      "[(('ta', 'ütles'), 7), (('ma', 'saan'), 5), (('ma', 'tean'), 4)]\n",
      "[(('մենք', 'տեսանք'), 6), (('ես', 'գիտեմ'), 5), (('ես', 'հասկանում'), 5)]\n",
      "[(('ego', 'subscripsi'), 1716), (('me', 'teste'), 1269), (('ego', 'conplevi'), 365)]\n",
      "[(('_', '_'), 21256), (('il', '_'), 504), ((\"c'\", '_'), 215)]\n",
      "[(('%', 'agua'), 52), (('%', 'pertenecían'), 51), (('%', 'afroamericanos'), 50)]\n",
      "[(('vi', 'har'), 113), (('jeg', 'har'), 93), (('han', 'sier'), 79)]\n",
      "[(('to', 'je'), 22), (('kaj', 'je'), 10), (('[name:personal]', 'je'), 8)]\n",
      "[(('es', 'domāju'), 18), (('kas', 'ir'), 18), (('es', 'nezinu'), 12)]\n",
      "[(('aš', 'dūstu'), 3), (('kurie', 'vadinami'), 2), (('tie', 'vadinami'), 2)]\n",
      "[(('وی', 'گفت'), 29), (('وی', 'کرد'), 28), (('من', 'می\\u200cکنم'), 25)]\n",
      "[(('_', '_'), 8368)]\n",
      "[(('iesus', 'dixit'), 74), (('iesus', 'ait'), 57), (('iesus', 'dicit'), 45)]\n",
      "[(('iesus', 'qaþ'), 64), (('is', 'qaþ'), 22), (('iesus', 'andhof'), 15)]\n",
      "[(('kdo', 'je'), 14), (('to', 'pomeni'), 14), (('to', 'zgodilo'), 11)]\n",
      "[(('i', 'like'), 43), (('everyone', 'has'), 22), (('we', 'have'), 10)]\n",
      "[(('que', 'tem'), 33), (('que', 'está'), 29), (('que', 'é'), 26)]\n",
      "[(('수', '있는'), 60), (('수', '있다'), 48), (('거', '같아요'), 14)]\n",
      "[(('ii', 'монархы'), 1), (('ii', 'шыққан'), 1), (('адам', 'жұмбақ'), 1)]\n",
      "[(('es', 'gibt'), 1296), (('es', 'hieß'), 564), (('es', 'heißt'), 512)]\n",
      "[(('hori', 'dela'), 12), (('lagun', 'hil'), 9), (('nik', 'dut'), 7)]\n",
      "[(('#labuonascuola', 'quella'), 3), (('#labuonascuola', 'scuola'), 3), (('chi', 'è'), 3)]\n",
      "[(('que', 'fai'), 3), (('que', 'ter'), 3), (('que', 'teñen'), 3)]\n",
      "[(('teikėjas', 'privalo'), 7), (('įmonė', 'turi'), 7), (('duomenys', 'tvarkomi'), 6)]\n",
      "[(('aboriya', 'bendavê'), 1), (('brîtanyayê', 'dagir kiriye'), 1), (('civaka', 'tê'), 1)]\n",
      "[(('vláda', 'pripraví'), 16), (('to', 'je'), 13), (('chris', 'odvetil'), 10)]\n",
      "[(('nous', 'devons'), 15), (('je', 'voudrais'), 14), (('je', 'pense'), 13)]\n",
      "[(('ئۇ', 'دېدى'), 9), (('بۆرە', 'كەلدى'), 6), (('ئۇ', 'ئويغاندى'), 5)]\n",
      "[(('것이', '아니라'), 35), (('필요가', '있다'), 34), (('것이', '좋다'), 19)]\n",
      "[(('ἀχιλλεύς', 'προσέφη'), 19), (('ζεύς', 'προσέφη'), 16), (('ῥωμαῖοι', 'κατέστησαν'), 14)]\n",
      "[(('nome', 'qual'), 36), (('che', 'hanno'), 33), (('presidente', 'chi'), 32)]\n",
      "[(('see', 'tähendab'), 49), (('ta', 'ütles'), 34), (('ma', 'tea'), 31)]\n",
      "[(('面积', '平方公里'), 52), (('人口', '人'), 41), (('高度', '米'), 14)]\n",
      "[(('холопъ', 'бьетъ'), 11), (('я', 'послалъ'), 6), (('ты', 'пожаловалъ'), 4)]\n",
      "[(('子', '曰'), 341), (('孟子', '曰'), 188), (('孔子', '曰'), 154)]\n",
      "[(('he', 'said'), 31), (('i', 'know'), 25), (('he', 'had'), 24)]\n",
      "[(('володимеръ', 'реч'), 20), (('кнѧзь', 'престави'), 19), (('ѡн', 'реч'), 18)]\n",
      "[(('han', 'hade'), 27), (('han', 'sa'), 26), (('han', 'gick'), 23)]\n",
      "[(('kerem', 'dedi'), 7), (('naci', 'bey'), 5), (('adam', 'dedi'), 4)]\n",
      "[(('er', 'wurde'), 70), (('es', 'handelt'), 34), (('es', 'gibt'), 32)]\n",
      "[(('_', '_'), 23130)]\n",
      "[(('quod', 'est'), 253), (('quae', 'sunt'), 206), (('quae', 'est'), 170)]\n",
      "[(('מקורות', 'מסרו'), 15), (('ספק', 'אין'), 15), (('הוא', 'אמר'), 9)]\n",
      "[((\"c'\", 'est'), 79), (('vous', 'allez'), 27), (('je', 'vais'), 23)]\n",
      "[(('yang', 'ada'), 66), (('yang', 'terletak'), 50), (('yang', 'berada'), 44)]\n",
      "[(('тэлеграф', 'перадаваў'), 5), (('я', 'магла'), 4), (('я', 'чула'), 3)]\n",
      "[(('se', 'on'), 29), (('se', 'oli'), 25), (('hän', 'tuli'), 22)]\n",
      "[(('to', 'je'), 25), (('co', 'stalo'), 21), (('co', 'je'), 15)]\n",
      "[(('பேர்', 'உயிரிழந்தனர்'), 4), (('வட்டாரங்கள்', 'தெரிவித்தன'), 4), (('அவர்', 'கூறினார்'), 3)]\n",
      "[(('to', 'znamená'), 59), (('to', 'je'), 57), (('které', 'mají'), 52)]\n",
      "[(('que', 'ten'), 13), (('que', 'fai'), 10), (('que', 'teñen'), 8)]\n",
      "[(('you', 'have'), 188), (('i', 'have'), 174), (('i', 'had'), 117)]\n",
      "[(('on', 'peut'), 63), (('il', 'fait'), 54), (('il', 'a'), 51)]\n",
      "[(('individuo', 'ha'), 23), (('che', 'consumano'), 6), (('che', 'rappresenta'), 4)]\n",
      "[(('_', '_'), 39943)]\n",
      "[(('pan', 'ma'), 19), (('to', 'znaczy'), 19), (('ja', 'mam'), 12)]\n"
     ]
    }
   ],
   "source": [
    "def corpora_extractor():\n",
    "    for file in files:\n",
    "        sentences = read_sentences(file)\n",
    "        formatted_corpus = split_rows(sentences, column_names_u)\n",
    "        formatted_corpus_dict = convert_to_dict(formatted_corpus)\n",
    "        frequent_pairs, frequent_triples = extract_pairs_and_triples(formatted_corpus_dict, nbest)\n",
    "        print(frequent_pairs)\n",
    "corpora_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your report, you will include the `nbest` most frequent pairs and triples you obtained in **three languages**. You may choose the ones you want.\n",
    "\n",
    "For the checking script, you will extract `nbest` triples in French, Russian, and English. You will rank these triples by frequency, and then by alphabetical order of the triple using `sorted()`. You will use the French GSD corpus, the Russian SynTagRus corpus, and the English EWT corpus. You will store these triples in the following variables:\n",
    "`freq_triples_fr`, `freq_triples_ru`, `freq_triples_en`. Each variable will contain a list of tuples: `(subject, verb, object), freq)`\n",
    "\n",
    "Here is what you should find:\n",
    "\n",
    "French\n",
    "```\n",
    "freq_triples_fr = [(('il', 'fait', 'partie'), 16),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "And the frequencies:\n",
    "```\n",
    "16\n",
    "7\n",
    "7\n",
    "```\n",
    "\n",
    "Russian:\n",
    "```\n",
    "freq_triples_ru = [(('мы', 'имеем', 'дело'), 6),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "And the frequencies:\n",
    "```\n",
    "6\n",
    "4\n",
    "4\n",
    "```\n",
    "\n",
    "English:\n",
    "```\n",
    "freq_triples_en = [(('you', 'have', 'questions'), 22),\n",
    "\n",
    " ...]\n",
    " ````\n",
    "\n",
    "And the frequencies:\n",
    "```\n",
    "22\n",
    "12\n",
    "7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [path_fr, path_ru, path_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_sentences(files[0])\n",
    "formatted_corpus = split_rows(sentences, column_names_u)\n",
    "formatted_corpus_dict = convert_to_dict(formatted_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('il', 'fait', 'partie'), 16),\n",
       " (('elle', 'fait', 'partie'), 7),\n",
       " (('il', 'comptait', 'habitants'), 7)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pairs_fr, freq_triples_fr = extract_pairs_and_triples(formatted_corpus_dict, nbest)\n",
    "freq_triples_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_sentences(files[1])\n",
    "formatted_corpus = split_rows(sentences, column_names_u)\n",
    "formatted_corpus_dict = convert_to_dict(formatted_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('мы', 'имеем', 'дело'), 6),\n",
       " (('мы', 'имеем', 'что'), 4),\n",
       " (('мы', 'сделаем', 'все'), 4)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pairs_ru, freq_triples_ru = extract_pairs_and_triples(formatted_corpus_dict, nbest)\n",
    "freq_triples_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_sentences(files[2])\n",
    "formatted_corpus = split_rows(sentences, column_names_u)\n",
    "formatted_corpus_dict = convert_to_dict(formatted_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('you', 'have', 'questions'), 22),\n",
       " (('you', 'think', 'what'), 12),\n",
       " (('i', 'do', 'what'), 7)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pairs_en, freq_triples_en = extract_pairs_and_triples(formatted_corpus_dict, nbest)\n",
    "freq_triples_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now extract the relations involving named entities, that is where both the subject and the object are proper nouns. \n",
    "\n",
    "Write an `extract_entity_triples(formatted_corpus_dict)` that will process the corpus and return a list of `(subject, verb, object)` triples. You will leave the case as it is in the form, for instance _United States_ and not _united states_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_triples(formatted_corpus_dict):\n",
    "    result = Counter()\n",
    "    \n",
    "    for sentence in formatted_corpus_dict:\n",
    "        subject_verb_object = []\n",
    "        for _id in sentence:\n",
    "            word = sentence[_id]\n",
    "            head_id = word['HEAD']\n",
    "            deprel = word[\"DEPREL\"].split(\":\")\n",
    "            upos = word[\"UPOS\"]\n",
    "            \n",
    "            if deprel[0] == \"nsubj\" and upos == \"PROPN\":\n",
    "                updated = False\n",
    "                head_form = sentence[head_id]['FORM']\n",
    "                for svo in subject_verb_object:\n",
    "                    if svo[\"verb\"] == head_form and not svo[\"subject\"]:\n",
    "                        svo[\"subject\"] = word['FORM']\n",
    "                        updated = True\n",
    "                        break\n",
    "                if not updated:\n",
    "                    subject_verb_object.append({\"subject\":word[\"FORM\"], \"verb\":head_form, \"object\":\"\"})\n",
    "                    \n",
    "            elif deprel[0] == \"obj\" and upos == \"PROPN\":\n",
    "                updated = False\n",
    "                head_form = sentence[head_id]['FORM']\n",
    "                for svo in subject_verb_object:\n",
    "                    if svo[\"verb\"] == head_form and not svo[\"object\"]:\n",
    "                        svo[\"object\"] = word['FORM']\n",
    "                        updated = True\n",
    "                        break\n",
    "                if not updated:\n",
    "                    subject_verb_object.append({\"subject\":\"\", \"verb\":head_form, \"object\":word[\"FORM\"]})\n",
    "\n",
    "        for el in subject_verb_object:\n",
    "            if el[\"subject\"] and el[\"verb\"] and el[\"object\"]:\n",
    "                result[(el[\"subject\"], el[\"verb\"], el[\"object\"])] += 1\n",
    "    return result.elements()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will run the `extract_entity_triples()` function one the English EWT corpus. You will store the list in the `entity_relation_en` variable and you will sort it with `sorted()`. You will keep the **five** first triples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two first triples are:\n",
    "```\n",
    "[('Baba', 'remember', 'George'),\n",
    " ('Beschta', 'told', 'Planet'),\n",
    "...]\n",
    " ```\n",
    "Note that this time, we keep the original case and the triples are in the alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = read_sentences(path_en)\n",
    "formatted_corpus = split_rows(sentences, column_names_u)\n",
    "formatted_corpus_dict = convert_to_dict(formatted_corpus)\n",
    "entity_relation_en = extract_entity_triples(formatted_corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Baba', 'remember', 'George'),\n",
       " ('Beschta', 'told', 'Planet'),\n",
       " ('Boi', 'beat', 'Lopez'),\n",
       " ('Bush', 'mentioned', 'Arabia'),\n",
       " ('Bush', 'mentioned', 'Osama')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_relation_en = sorted(entity_relation_en)[:nbest]\n",
    "entity_relation_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: Extracting the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting only the headword of the subject and object if often incomplete and uninformative. You can extract all the chunk instead. As an optional exercise, you can try a baseline technique and extract adjacent proper nouns. You may also want to apply the chunker of the 4th assignment to the corpus to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: Mapping the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the chunker assignment, you may also want to complement your assignment with a entity solver that will link the entities to wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the article: _PRISMATIC: Inducing Knowledge from a Large Scale Lexicalized Relation Resource_ by Fan and al. (2010) [<a href=\"http://www.aclweb.org/anthology/W/W10/W10-0915.pdf\">pdf</a>] and write in a few sentences how it relates to your work in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"jo1360lo-s\"] # Write your stil ids as a list\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"5-triples.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the pairs and triples in four languages, as well as the triples with named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"freq_triples_sv\": [[[\"man\", \"v\\\\u00e4nder\", \"sig\"], 14], [[\"det\", \"r\\\\u00f6r\", \"sig\"], 5], [[\"man\", \"s\\\\u00f6ker\", \"arbete\"], 3]], \"freq_triples_fr\": [[[\"il\", \"fait\", \"partie\"], 16], [[\"elle\", \"fait\", \"partie\"], 7], [[\"il\", \"comptait\", \"habitants\"], 7]], \"freq_triples_ru\": [[[\"\\\\u043c\\\\u044b\", \"\\\\u0438\\\\u043c\\\\u0435\\\\u0435\\\\u043c\", \"\\\\u0434\\\\u0435\\\\u043b\\\\u043e\"], 6], [[\"\\\\u043c\\\\u044b\", \"\\\\u0438\\\\u043c\\\\u0435\\\\u0435\\\\u043c\", \"\\\\u0447\\\\u0442\\\\u043e\"], 4], [[\"\\\\u043c\\\\u044b\", \"\\\\u0441\\\\u0434\\\\u0435\\\\u043b\\\\u0430\\\\u0435\\\\u043c\", \"\\\\u0432\\\\u0441\\\\u0435\"], 4]], \"freq_triples_en\": [[[\"you\", \"have\", \"questions\"], 22], [[\"you\", \"think\", \"what\"], 12], [[\"i\", \"do\", \"what\"], 7]], \"entity_relation_en\": [[\"Baba\", \"remember\", \"George\"], [\"Beschta\", \"told\", \"Planet\"], [\"Boi\", \"beat\", \"Lopez\"], [\"Bush\", \"mentioned\", \"Arabia\"], [\"Bush\", \"mentioned\", \"Osama\"]]}'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "ANSWER = json.dumps({'freq_triples_sv': freq_triples_sv,\n",
    "                     'freq_triples_fr': freq_triples_fr,\n",
    "                     'freq_triples_ru': freq_triples_ru,\n",
    "                     'freq_triples_en': freq_triples_en,\n",
    "                     'entity_relation_en': entity_relation_en\n",
    "                    })\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "ASSIGNMENT = 5\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': None,\n",
       " 'status': 'correct',\n",
       " 'signature': '8c77fafa059d70f9c958c36fbc896151a5f8ec5cd730f4a3484b706363dacc179f12aa2e5e35798c20fd7981fa56b3babd7423e0aaa53a88400dff81f308b674',\n",
       " 'submission_id': 'ae071060-1bc1-415c-bdbd-a0894acb49e0'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "               verify=True)\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
